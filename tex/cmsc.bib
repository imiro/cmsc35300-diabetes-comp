
@article{smote,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {16},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ``normal'' examples with only a small percentage of ``abnormal'' or ``interesting'' examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  langid = {english},
  file = {/Users/imiro/Zotero/storage/ZADZULS6/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf}
}

@article{foraker,
  title = {The {{National COVID Cohort Collaborative}}: {{Analyses}} of {{Original}} and {{Computationally Derived Electronic Health Record Data}}},
  shorttitle = {The {{National COVID Cohort Collaborative}}},
  author = {Foraker, Randi and Guo, Aixia and Thomas, Jason and Zamstein, Noa and Payne, Philip RO and Wilcox, Adam and Collaborative, N3c},
  year = {2021},
  month = oct,
  journal = {Journal of Medical Internet Research},
  volume = {23},
  number = {10},
  pages = {e30697},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/30697},
  abstract = {Background: Computationally derived (``synthetic'') data can enable the creation and analysis of clinical, laboratory, and diagnostic data as if they were the original electronic health record data. Synthetic data can support data sharing to answer critical research questions to address the COVID-19 pandemic. Objective: We aim to compare the results from analyses of synthetic data to those from original data and assess the strengths and limitations of leveraging computationally derived data for research purposes. Methods: We used the National COVID Cohort Collaborative's instance of MDClone, a big data platform with data-synthesizing capabilities (MDClone Ltd). We downloaded electronic health record data from 34 National COVID Cohort Collaborative institutional partners and tested three use cases, including (1) exploring the distributions of key features of the COVID-19\textendash positive cohort; (2) training and testing predictive models for assessing the risk of admission among these patients; and (3) determining geospatial and temporal COVID-19\textendash related measures and outcomes, and constructing their epidemic curves. We compared the results from synthetic data to those from original data using traditional statistics, machine learning approaches, and temporal and spatial representations of the data. Results: For each use case, the results of the synthetic data analyses successfully mimicked those of the original data such that the distributions of the data were similar and the predictive models demonstrated comparable performance. Although the synthetic and original data yielded overall nearly the same results, there were exceptions that included an odds ratio on either side of the null in multivariable analyses (0.97 vs 1.01) and differences in the magnitude of epidemic curves constructed for zip codes with low population counts. Conclusions: This paper presents the results of each use case and outlines key considerations for the use of synthetic data, examining their role in collaborative research for faster insights.},
  langid = {english},
  file = {/Users/imiro/Zotero/storage/MSDRIT2V/Foraker et al_2021_The National COVID Cohort Collaborative.pdf;/Users/imiro/Zotero/storage/7RHCRJZQ/e30697.html}
}

@article{kavakiotis,
  title = {Machine {{Learning}} and {{Data Mining Methods}} in {{Diabetes Research}}},
  author = {Kavakiotis, Ioannis and Tsave, Olga and Salifoglou, Athanasios and Maglaveras, Nicos and Vlahavas, Ioannis and Chouvarda, Ioanna},
  year = {2017},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {15},
  pages = {104--116},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2016.12.005},
  abstract = {The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85\% of those used were characterized by supervised learning approaches and 15\% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM.},
  langid = {english},
  keywords = {Biomarker(s) identification,Data mining,Diabetes mellitus,Diabetic complications,Disease prediction models,Machine learning},
  file = {/Users/imiro/Zotero/storage/YXJ6R4HP/Kavakiotis et al_2017_Machine Learning and Data Mining Methods in Diabetes Research.pdf;/Users/imiro/Zotero/storage/B6S5J9YW/S2001037016300733.html}
}

@article{leesnomedct,
  title = {Literature Review of {{SNOMED CT}} Use},
  author = {Lee, Dennis and {de Keizer}, Nicolette and Lau, Francis and Cornet, Ronald},
  year = {2014},
  month = feb,
  journal = {Journal of the American Medical Informatics Association},
  volume = {21},
  number = {e1},
  pages = {e11-9},
  issn = {1527-974X},
  doi = {10.1136/amiajnl-2013-001636},
  abstract = {ObjectiveThe aim of this paper is to report on the use of the systematised nomenclature of medicine clinical terms (SNOMED CT) by providing an overview of published papers.MethodsPublished papers on SNOMED CT between 2001 and 2012 were identified using PubMed and Embase databases using the keywords 'systematised nomenclature of medicine' and 'SNOMED CT'. For each paper the following characteristics were retrieved: SNOMED CT focus category (ie, indeterminate, theoretical, pre-development/design, implementation and evaluation/commodity), usage category (eg, prospective content coverage, used to classify or code in a study), medical domain and country.ResultsOur search strategy identified 488 papers. A comparison between the papers published between 2001-6 and 2007-12 showed an increase in every SNOMED CT focus category. The number of papers classified as 'theoretical' increased from 46 to 78, 'pre-development/design' increased from 61 to 173 and 'implementation' increased from 10 to 34. Papers classified as 'evaluation/commodity' only started to appear from 2010.ConclusionsThe majority of studies focused on 'theoretical' and 'pre-development/design'. This is still encouraging as SNOMED CT is being harmonized with other standardized terminologies and is being evaluated to determine the content coverage of local terms, which is usually one of the first steps towards adoption. Most implementations are not published in the scientific literature, requiring a look beyond the scientific literature to gain insights into SNOMED CT implementations.},
  langid = {english},
  pmcid = {PMC3957381},
  pmid = {23828173},
  keywords = {Controlled Vocabularies,Implementation,Literature review,Snomed Ct}
}

@article{ravaut21,
  title = {Predicting Adverse Outcomes Due to Diabetes Complications with Machine Learning Using Administrative Health Data},
  author = {Ravaut, Mathieu and Sadeghi, Hamed and Leung, Kin Kwan and Volkovs, Maksims and Kornas, Kathy and Harish, Vinyas and Watson, Tristan and Lewis, Gary F. and Weisman, Alanna and Poutanen, Tomi and Rosella, Laura},
  year = {2021},
  month = feb,
  journal = {npj Digital Medicine},
  volume = {4},
  number = {1},
  pages = {1--12},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-021-00394-8},
  abstract = {Across jurisdictions, government and health insurance providers hold a large amount of data from patient interactions with the healthcare system. We aimed to develop a machine learning-based model for predicting adverse outcomes due to diabetes complications using administrative health data from the single-payer health system in Ontario, Canada. A Gradient Boosting Decision Tree model was trained on data from 1,029,366 patients, validated on 272,864 patients, and tested on 265,406 patients. Discrimination was assessed using the AUC statistic and calibration was assessed visually using calibration plots overall and across population subgroups. Our model predicting three-year risk of adverse outcomes due to diabetes complications (hyper/hypoglycemia, tissue infection, retinopathy, cardiovascular events, amputation) included 700 features from multiple diverse data sources and had strong discrimination (average test AUC\,=\,77.7, range 77.7\textendash 77.9). Through the design and validation of a high-performance model to predict diabetes complications adverse outcomes at the population level, we demonstrate the potential of machine learning and administrative health data to inform health planning and healthcare resource allocation for diabetes management.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Epidemiology,Health services},
  file = {/Users/imiro/Zotero/storage/2LGJCW5T/Ravaut et al_2021_Predicting adverse outcomes due to diabetes complications with machine learning.pdf;/Users/imiro/Zotero/storage/JHDUWBLQ/s41746-021-00394-8.html}
}

@misc{VHA,
  title = {{{VHA Innovation Ecosystem}} and {{precisionFDA COVID-19 Risk Factor Modeling Challenge Phase}} 1 - {{PrecisionFDA Challenge}}},
  howpublished = {https://precision.fda.gov/challenges/11},
  file = {/Users/imiro/Zotero/storage/PP6M522C/11.html}
}

@article{synthea,
  title = {Synthea: {{An}} Approach, Method, and Software Mechanism for Generating Synthetic Patients and the Synthetic Electronic Health Care Record},
  author = {Walonoski, Jason and Kramer, Mark and Nichols, Joseph and Quina, Andre and Moesel, Chris and Hall, Dylan and Duffett, Carlton and Dube, Kudakwashe and Gallagher, Thomas and McLachlan, Scott},
  year = {2018},
  month = mar,
  journal = {Journal of the American Medical Informatics Association},
  volume = {25},
  number = {3},
  pages = {230--238},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocx079},
  abstract = {Our objective is to create a source of synthetic electronic health records that is readily available; suited to industrial, innovation, research, and educational uses; and free of legal, privacy, security, and intellectual property restrictions.We developed Synthea, an open-source software package that simulates the lifespans of synthetic patients, modeling the 10 most frequent reasons for primary care encounters and the 10 chronic conditions with the highest morbidity in the United States.Synthea adheres to a previously developed conceptual framework, scales via open-source deployment on the Internet, and may be extended with additional disease and treatment modules developed by its user community. One million synthetic patient records are now freely available online, encoded in standard formats (eg, Health Level-7 [HL7] Fast Healthcare Interoperability Resources [FHIR] and Consolidated-Clinical Document Architecture), and accessible through an HL7 FHIR application program interface.Health care lags other industries in information technology, data exchange, and interoperability. The lack of freely distributable health records has long hindered innovation in health care. Approaches and tools are available to inexpensively generate synthetic health records at scale without accidental disclosure risk, lowering current barriers to entry for promising early-stage developments. By engaging a growing community of users, the synthetic data generated will become increasingly comprehensive, detailed, and realistic over time.Synthetic patients can be simulated with models of disease progression and corresponding standards of care to produce risk-free realistic synthetic health care records at scale.}
}


